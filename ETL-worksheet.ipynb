{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DEMO_KEY` should work for a small number of requests, or request a key at https://api.data.gov/signup/.\n",
    "\n",
    "The first block requests the [document](https://www.regulations.gov/docket?D=DOI-2017-0002) about Bears Ear National Monument that elicited 1.5 million comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_KEY = 'DEMO_KEY'\n",
    "path = 'https://api.data.gov/regulations/v3/document.json'\n",
    "query = {'documentId':'DOI-2017-0002-0001', 'api_key':API_KEY}\n",
    "response = requests.get(path, params=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract data from the returned JSON object, which gets mapped to a Python dictionary called `doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Comments Received: 2839046\n"
     ]
    }
   ],
   "source": [
    "doc = response.json()\n",
    "print('{}: {}'.format(\n",
    "    doc['numItemsRecieved']['label'],\n",
    "    doc['numItemsRecieved']['value'],\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate a new API query for public submission (PS) comments and print the dictionary keys in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['documents', 'totalNumRecords']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = {\n",
    "    'dktid': doc['docketId']['value'],\n",
    "    'dct': 'PS',\n",
    "    'api_key': API_KEY}\n",
    "path = 'https://api.data.gov/regulations/v3/documents.json'\n",
    "response = requests.get(path, params=query)\n",
    "dkt = response.json()\n",
    "list(dkt.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purported claimed number of results is much larger than the length of the documents array contained in this response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Number received: {}\\nTotal number: {}'.format(\n",
    "    len(dkt['documents']),\n",
    "    dkt['totalNumRecords'],\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll have to flip through \"pages\" with the API and store the response at each iteration.\n",
    "\n",
    "The next block runs a system command to initialize an empty database called \"BENM\", this can only be done once and would typically be performed by a database administrator (which may, in fact, be you!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from subprocess import check_output\n",
    "check_output('echo $(whoami) | sudo -u postgres -S createdb BENM', shell=True) == b''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following commands prepare Python to connect to the database, and creates empty tables in the database if they do not already exist (i.e. it is safe to re-run after you have populated the database).\n",
    "\n",
    "**Step 1: Boilerplate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy import create_engine, Column\n",
    "from sqlalchemy.dialects.postgresql import BIGINT, JSONB\n",
    "\n",
    "Base = declarative_base()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Table Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Comment(Base):\n",
    "    __tablename__ = 'comment'\n",
    "    \n",
    "    id = Column(BIGINT, primary_key=True)\n",
    "    json = Column(JSONB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Connect and initialize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql+pygresql://@localhost/BENM')\n",
    "Session = sessionmaker(bind=engine)\n",
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could inspect the BENM database hosted by \"localhost\" now in the PostgreSQL Studio app; you would find one empty \"comment\" table with fields \"id\" and \"json\".\n",
    "\n",
    "Add a new `rpp` parameter to request `100` documents per page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = {\n",
    "    'dktid': doc['docketId']['value'],\n",
    "    'dct': 'PS',\n",
    "    'rpp': 100,\n",
    "    'api_key': API_KEY,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each request, advance the query parameter `po` to the number of the record you want the response to begin with. Insert the documents (the key:value pairs stored in `values`) in bulk to the database with `engine.execute()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, 10):\n",
    "    query['po'] = i * query['rpp']\n",
    "    print(query['po'])\n",
    "    response = requests.get(path, params=query)\n",
    "    page = response.json()\n",
    "    docs = page['documents']\n",
    "    values = [{'json': json} for json in docs]\n",
    "    insert = Comment.__table__.insert().values(values)\n",
    "    engine.execute(insert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what the first document in the database looks like. Or set `id` to a different value to look at other comments in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = Session()\n",
    "id = 1\n",
    "comment = s.query(Comment).filter_by(id=id).first()\n",
    "print('id: {}\\ntext: {}'.format(\n",
    "    comment.id,\n",
    "    comment.json['commentText']\n",
    "))\n",
    "s.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that some of the comments appear to be identical, probably some form e-mail the public was pasting into the comment field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = Session()\n",
    "q = s.query(Comment.id, Comment.json['commentText'].label('text')).limit(30);\n",
    "for r in q:\n",
    "    print('{}: {}'.format(r.id, r.text[:75].replace('\\n', ' ')))\n",
    "s.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin pre-processing the texts. In the block below we rely on some \"in-database\" operations on the assumption that the data are too big to read into memory for processing in our Python process. The product is a new database table called `doc` with the following fields:\n",
    "\n",
    "1. A new `id` for each unique comment text.\n",
    "1. The number of duplicated comments per each `id`, labeled `rep`.\n",
    "1. A `tokens` field containing a condensed version of the comment text as a list of stemmed words, excluding stopwords, and their positions in the comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = Session()\n",
    "s.execute('''\n",
    "    WITH dedupe AS (\n",
    "        SELECT count(*) AS rep, to_tsvector(json ->> 'commentText') AS tokens\n",
    "        FROM comment\n",
    "        GROUP BY tokens)\n",
    "    SELECT row_number() OVER (ORDER BY rep) AS id, rep, tokens\n",
    "    INTO TABLE doc\n",
    "    FROM dedupe;\n",
    "    ''')\n",
    "s.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our final step in ETL (the \"Load\" step), we do an in-database conversion of the data into an easier form for analysis. The new table `word` will have the `id` from the `doc` table, each `word` in that document and its `freq`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s.execute('''\n",
    "    SELECT id AS doc_id, word, nentry AS freq\n",
    "    INTO TABLE word\n",
    "    FROM doc\n",
    "    JOIN ts_stat('select tokens from doc where id = ' || id) ON true;\n",
    "''')\n",
    "s.commit()\n",
    "s.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to disconnect from your database!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "engine.dispose()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
